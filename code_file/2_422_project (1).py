# -*- coding: utf-8 -*-
"""2. 422 project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XbKT4paxgjASKmtXOg4aXEnwrhxUnBr1
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

shipping=pd.read_csv("https://raw.githubusercontent.com/Jaky-Chow02/Academics/refs/heads/main/E-commerce%20Shipping%20Dataset.csv")

shipping.head()

shipping["Reached.on.Time_Y.N"].unique()

print(shipping.groupby("Reached.on.Time_Y.N").size())

shipping.shape

shipping_2=shipping.drop("Reached.on.Time_Y.N", axis=1)

categorical_data=shipping_2.select_dtypes(include='object')
categorical_features=categorical_data.columns.tolist()
print(f'There are {len(categorical_features)} categorical features:', '\n')
print(categorical_features)

numerical_data=shipping_2.select_dtypes(include='number')
numerical_features=numerical_data.columns.tolist()
print(f'There are {len(numerical_features)} numerical features:', '\n')
print(numerical_features)

x=shipping["Reached.on.Time_Y.N"].unique()
y=print(x)
z=[]
for i in x:
  z.append(int(i))

class_counts=shipping.groupby("Reached.on.Time_Y.N").size()
class_counts_value=len(class_counts)
columns=["reached_on_Time_YN","count","percentage"]
reached_on_Time_YN=z
count=list()
percentage=list()
for i in range(class_counts_value):
  count.append(class_counts[i])
  percent=(class_counts[i]/shipping.shape[0])*100
  percentage.append(percent)
imbalance_df=pd.DataFrame(list(zip(reached_on_Time_YN,count,percentage)),columns=columns)
imbalance_df

import seaborn as sns
sns.barplot(data=imbalance_df,x=imbalance_df['reached_on_Time_YN'],y=imbalance_df['count'])
plt.show()

correlation_matrix = numerical_data.corr()
correlation_matrix

plt.figure(figsize=(8, 6))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.3f', linewidths=0.3)
plt.show()

sns.heatmap(correlation_matrix, cmap = 'YlGnBu')

for col in categorical_features:
    plt.title(f'Distribution of {col}')
    categorical_data[col].value_counts().sort_index().plot(kind='bar', rot=0, xlabel=col,ylabel='count')
    plt.show()

shipping.isnull().sum()

shipping['Warehouse_block'].unique()

shipping["Warehouse_block"]=shipping["Warehouse_block"].map({'D':0 , 'F':1, 'A':2, 'B':3, 'C':4})

shipping.head()

shipping["Mode_of_Shipment"]=shipping["Mode_of_Shipment"].map({"Flight":0, "Ship":1, "Road":2})

shipping.head()

shipping["Product_importance"].unique()

shipping["Product_importance"]=shipping["Product_importance"].map({"low":0, "medium":1, "high":2})

shipping.head()

shipping["Gender"].unique()

from sklearn.preprocessing import LabelEncoder
enc=LabelEncoder()
shipping["Gender"]=enc.fit_transform(shipping["Gender"])

shipping.head()

shipping.isnull().sum()

"""Creating a dummy column with mostly NaN values"""

shipping["Dummy_val"]=np.nan

shipping.isnull().sum()

shipping=shipping.drop("Dummy_val",axis=1)

shipping.isnull().sum()

shipping=shipping.drop("ID",axis=1)

shipping.isnull().sum()

shipping.head()

from sklearn.model_selection import train_test_split

shipping_1=shipping.drop("Reached.on.Time_Y.N",axis=1)
X= shipping_1.values
y = shipping["Reached.on.Time_Y.N"].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
print(X_train.shape, X_test.shape)

from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors=7)
knn.fit(X_train, y_train)
print("Test accuracy without scaling: {:.2f}".format(knn.score(X_test, y_test)))

from sklearn.preprocessing import StandardScaler

scaler_std = StandardScaler()
X_train_scaled = scaler_std.fit_transform(X_train)
X_test_scaled = scaler_std.transform(X_test)

knn_std = KNeighborsClassifier(n_neighbors=7)
knn_std.fit(X_train_scaled, y_train)
print("Test accuracy with StandardScaler: {:.2f}".format(knn_std.score(X_test_scaled, y_test)))

"""# **Model training**

## Supervised models : KNN, Logistic Regression, Decision Tree
"""

#KNN
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, roc_auc_score, roc_curve

knn=KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_scaled, y_train.ravel())

y_pred_knn=knn.predict(X_test_scaled)
y_prob_knn=knn.predict_proba(X_test_scaled)[:,1]

print("---------------------------------")
print("KNN Accuracy :")
print(accuracy_score(y_test, y_pred_knn))
print("---------------------------------")
print("KNN Precision:")
print(precision_score(y_test, y_pred_knn))
print("---------------------------------")
print("KNN Recall:")
print(recall_score(y_test, y_pred_knn))
print("---------------------------------")
print("KNN AUC: ")
print(roc_auc_score(y_test, y_prob_knn))
print("---------------------------------")

cm_knn = confusion_matrix(y_test, y_pred_knn)
fpr_knn, tpr_knn, _ = roc_curve(y_test, y_prob_knn)

from sklearn.linear_model import LogisticRegression

log_reg=LogisticRegression(max_iter=1000, random_state=1)
log_reg.fit(X_train_scaled, y_train.ravel())
y_pred_log=log_reg.predict(X_test_scaled)
y_prob_log=log_reg.predict_proba(X_test_scaled)[:,1]

print("---------------------------------")
print("LogReg Accuracy :")
print(accuracy_score(y_test, y_pred_log))
print("---------------------------------")
print("LogReg Precision:")
print(precision_score(y_test, y_pred_log))
print("---------------------------------")
print("LogReg Recall:")
print(recall_score(y_test, y_pred_log))
print("---------------------------------")
print("LogReg AUC: ")
print(roc_auc_score(y_test, y_prob_log))
print("---------------------------------")

cm_log = confusion_matrix(y_test, y_pred_log)
fpr_log, tpr_log, _ = roc_curve(y_test, y_prob_log)

results={"KNN":{"acc": accuracy_score(y_test, y_pred_knn),"precision": precision_score(y_test, y_pred_knn),"recall": recall_score(y_test, y_pred_knn),"auc": roc_auc_score(y_test, y_prob_knn),"cm": cm_knn,"fpr_tpr": (fpr_knn, tpr_knn)},
    "LogReg": {"acc": accuracy_score(y_test, y_pred_log),"precision": precision_score(y_test, y_pred_log),"recall": recall_score(y_test, y_pred_log),"auc": roc_auc_score(y_test, y_prob_log),"cm": cm_log,"fpr_tpr": (fpr_log, tpr_log), } }

"""# Neural **Networks**"""

# import numpy as np
# from sklearn.metrics import (
#     accuracy_score, precision_score, recall_score,
#     roc_auc_score, confusion_matrix, roc_curve
# )

# # ===========================
# # 1. Activation, loss, optimizer
# # ===========================

# class ReLU:
#     # forward pass
#     def forwardPropagation(self, inp: np.array) -> np.array:
#         self.inputAct = np.maximum(0, inp)
#         return self.inputAct

#     # derivative for backprop
#     def backPropagation(self, delta: np.array) -> np.array:
#         return delta * np.where(self.inputAct > 0, 1, 0)


# class MeanSquareError:
#     def loss(self, Y: np.array, y: np.array) -> np.array:
#         self.Y = Y
#         self.y = y
#         return np.absolute((Y - y) ** 2).sum() / y.shape[0]

#     # dL/dY
#     def deriv(self) -> np.array:
#         return (self.Y - self.y) / self.y.shape[0]


# class GradientDescent:
#     def __init__(self, learningRate) -> None:
#         self.learningRate = learningRate

#     # scale gradients
#     def gradients(self, gradients) -> np.array:
#         return self.learningRate * gradients


# # ===========================
# # 2. Helper getters
# # ===========================

# def getActivationFunction(activation: str):
#     if activation == "RELU":
#         return ReLU
#     else:
#         raise Exception("Cannot find the activation function.")


# def getLossFunction(loss: str):
#     if loss == "MSE":
#         return MeanSquareError
#     else:
#         raise Exception("Cannot find the loss function.")


# def getOptimizer(optimizer: str):
#     if optimizer == "GD":
#         return GradientDescent
#     else:
#         raise Exception("Cannot find the optimizer.")


# # ===========================
# # 3. Single layer of neurons
# # ===========================

# class NeuronLayer:
#     def __init__(self, inputNeurons, outputNeurons, activation, biasFlag=True, randomState=42) -> None:
#         np.random.seed(randomState)

#         self.inputNeurons = inputNeurons
#         self.outputNeurons = outputNeurons
#         self.biasFlag = biasFlag

#         # activation
#         self.activation = getActivationFunction(activation)()

#         # Xavier-like uniform init
#         self.weights = self.__initParameters((self.inputNeurons, self.outputNeurons))
#         self.bias = self.__initParameters((1, self.outputNeurons))

#     def __initParameters(self, dimension: tuple) -> np.array:
#         return np.random.uniform(
#             -np.sqrt(2 / (dimension[0] + dimension[1])),
#             np.sqrt(2 / (dimension[0] + dimension[1])),
#             size=(dimension[0], dimension[1])
#         )

#     def build(self, optimizer, learningRate) -> None:
#         self.learningRate = learningRate
#         self.optimizer = getOptimizer(optimizer)(learningRate)

#     # forward pass
#     def forwardPropagation(self, X: np.array) -> np.array:
#         self.X = X
#         self.output = np.dot(self.X, self.weights) + (self.biasFlag * self.bias)
#         self.output = self.activation.forwardPropagation(self.output)
#         return self.output

#     # backprop for this layer
#     def backPropagation(self, upstreamGradient: np.array) -> np.array:
#         # activation gradient
#         delta = self.activation.backPropagation(upstreamGradient)

#         # parameter gradients
#         weightGrad = np.dot(self.X.T, delta) / self.X.shape[0]
#         biasGrad = np.dot(np.ones((1, self.X.shape[0])), delta) / self.X.shape[0]

#         # update params
#         self.weights -= self.optimizer.gradients(weightGrad)
#         self.bias -= self.optimizer.gradients(biasGrad)

#         # downstream gradient
#         downstreamGradient = np.dot(delta, self.weights.T) / self.X.shape[0]

#         return downstreamGradient


# # ===========================
# # 4. Model class (multi-layer)
# # ===========================

# class Model:
#     # create mini-batches
#     def __createBatch(self, X: np.array, Y: np.array, batchSize: int) -> tuple:
#         miniX, miniY = np.array([X[:batchSize]]), np.array([Y[:batchSize]])

#         for idx in range(1, X.shape[0] // batchSize):
#             miniX = np.append(miniX, np.array([X[idx * batchSize: (idx + 1) * batchSize]]), axis=0)
#             miniY = np.append(miniY, np.array([Y[idx * batchSize: (idx + 1) * batchSize]]), axis=0)

#         return miniX, miniY

#     # forward through all layers
#     def __forwardPropagation(self, X: np.array) -> np.array:
#         output = X
#         for layer in self.layers:
#             output = layer.forwardPropagation(output)
#         return output

#     # backward through all layers
#     def __backPropagation(self, Y: np.array) -> None:
#         gradient = Y
#         for layer in self.layers[::-1]:
#             gradient = layer.backPropagation(gradient)

#     def layers(self, layers: list) -> None:
#         self.layers = layers

#     # compile model
#     def compile(self, loss, optimizer, learningRate) -> None:
#         self.loss = getLossFunction(loss)()
#         for layer in self.layers[::-1]:
#             layer.build(optimizer, learningRate)

#     # predict
#     def predict(self, X: np.array) -> np.array:
#         return self.__forwardPropagation(X)

#     # evaluate loss
#     def evaluate(self, X: np.array, Y: np.array) -> np.array:
#         output = self.predict(X)
#         return self.loss.loss(output, Y)

#     # train
#     def fit(self, X: np.array, Y: np.array, epochs: int, batchSize=None) -> np.array:
#         batchSize = (batchSize if batchSize else X.shape[0])
#         self.X, self.Y = self.__createBatch(X, Y, batchSize)

#         self.error = np.array([])
#         for epoch in range(epochs):
#             epochError = np.array([])

#             for idx in range(self.X.shape[0]):
#                 out = self.__forwardPropagation(self.X[idx])
#                 epochError = np.append(epochError, self.loss.loss(out, self.Y[idx]))
#                 self.__backPropagation(self.loss.deriv())

#             epochError /= epochError.shape[0]
#             self.error = np.append(self.error, epochError.sum() / epochError.shape[0])
#             print("Epoch:", epoch + 1, "Error:", round(epochError[0], 4))

#         return self.error


# # ===========================
# # 5. Use this model on your shipping data
# # ===========================

# # 5.1 Prepare arrays (y_train, y_test are already numpy arrays)
# X_train_arr = X_train_scaled
# X_test_arr  = X_test_scaled

# y_train_arr = y_train.reshape(-1, 1).astype(float)
# y_test_arr  = y_test.reshape(-1, 1).astype(float)

# # 5.2 Build network
# input_dim = X_train_arr.shape[1]

# nn_model = Model()

# layers = [
#     NeuronLayer(inputNeurons=input_dim, outputNeurons=32, activation="RELU"),
#     NeuronLayer(inputNeurons=32,        outputNeurons=16, activation="RELU"),
#     NeuronLayer(inputNeurons=16,        outputNeurons=1,  activation="RELU")
# ]

# nn_model.layers(layers)

# nn_model.compile(
#     loss="MSE",
#     optimizer="GD",
#     learningRate=0.001
# )

# # 5.3 Train
# errors = nn_model.fit(
#     X_train_arr,
#     y_train_arr,
#     epochs=30,
#     batchSize=64
# )

# # 5.4 Predict and evaluate
# y_pred_nn = nn_model.predict(X_test_arr)             # shape (n_test, 1)
# y_pred_labels = (y_pred_nn >= 0.5).astype(int)       # threshold

# acc_nn  = accuracy_score(y_test_arr, y_pred_labels)
# prec_nn = precision_score(y_test_arr, y_pred_labels)
# rec_nn  = recall_score(y_test_arr, y_pred_labels)
# auc_nn  = roc_auc_score(y_test_arr, y_pred_nn)

# cm_nn = confusion_matrix(y_test_arr, y_pred_labels)
# fpr_nn, tpr_nn, _ = roc_curve(y_test_arr, y_pred_nn)

# print("Custom NN Accuracy :", acc_nn)
# print("Custom NN Precision:", prec_nn)
# print("Custom NN Recall   :", rec_nn)
# print("Custom NN AUC      :", auc_nn)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

nn = Sequential([Dense(32, activation="relu", input_shape=(X_train_scaled.shape[1],)),Dense(16, activation="relu"),Dense(1, activation="sigmoid")])
nn.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])

history = nn.fit(X_train_scaled, y_train,validation_split=0.2,epochs=20,batch_size=64,verbose=1)

y_prob_nn = nn.predict(X_test_scaled).ravel()
y_pred_nn = (y_prob_nn >= 0.5).astype(int)

print("---------------------------------")
print("NeuralNet Accuracy :", accuracy_score(y_test, y_pred_nn))
print("---------------------------------")
print("NeuralNet Precision:", precision_score(y_test, y_pred_nn))
print("---------------------------------")
print("NeuralNet Recall   :", recall_score(y_test, y_pred_nn))
print("---------------------------------")
print("NeuralNet AUC      :", roc_auc_score(y_test, y_prob_nn))

cm_nn = confusion_matrix(y_test, y_pred_nn)
fpr_nn, tpr_nn, _ = roc_curve(y_test, y_prob_nn)

import seaborn as sns
import matplotlib.pyplot as plt

plt.figure(figsize=(4,3))
sns.heatmap(cm_log, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

plt.figure(figsize=(4,3))
sns.heatmap(cm_log, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - Logistic Regression")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()
plt.figure(figsize=(4,3))
sns.heatmap(cm_knn, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - KNN")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

plt.figure(figsize=(4,3))
sns.heatmap(cm_nn, annot=True, fmt="d", cmap="Blues")
plt.title("Confusion Matrix - Neural Networks")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

plt.figure(figsize=(5,4))
plt.plot(fpr_log, tpr_log, label=f"LogReg (AUC={roc_auc_score(y_test, y_prob_log):.2f})")
plt.plot([0,1], [0,1], "k--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Logistic Regression")
plt.legend()
plt.show()
plt.figure(figsize=(5,4))
plt.plot(fpr_knn, tpr_knn, label=f"LogReg (AUC={roc_auc_score(y_test, y_prob_log):.2f})")
plt.plot([0,1], [0,1], "k--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - KNN")
plt.legend()
plt.show()

plt.figure(figsize=(5,4))
plt.plot(fpr_nn, tpr_nn, label=f"LogReg (AUC={roc_auc_score(y_test, y_prob_log):.2f})")
plt.plot([0,1], [0,1], "k--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve - Neural Networks")
plt.legend()
plt.show()

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=2, random_state=42)
clusters = kmeans.fit_predict(X_train_scaled)

plt.figure(figsize=(5,4))
plt.scatter(X_train_scaled[:, 0], X_train_scaled[:, 1], c=clusters, cmap="viridis", s=5)
plt.title("KMeans Clusters (First 2 Features)")
plt.xlabel("Feature 1")
plt.ylabel("Feature 2")
plt.show()

# Compare clusters with true labels (optional)
cluster_vs_label = pd.crosstab(clusters, y_train.ravel(),rownames=["Cluster"], colnames=["Reached.on.Time_Y.N"])
print(cluster_vs_label)

results = {
    "KNN": {
        "acc": accuracy_score(y_test, y_pred_knn),
        "precision": precision_score(y_test, y_pred_knn),
        "recall": recall_score(y_test, y_pred_knn),
        "auc": roc_auc_score(y_test, y_prob_knn),
    },
    "LogReg": {
        "acc": accuracy_score(y_test, y_pred_log),
        "precision": precision_score(y_test, y_pred_log),
        "recall": recall_score(y_test, y_pred_log),
        "auc": roc_auc_score(y_test, y_prob_log),
    },

    "NeuralNet": {
        "acc": accuracy_score(y_test, y_pred_nn),
        "precision": precision_score(y_test, y_pred_nn),
        "recall": recall_score(y_test, y_pred_nn),
        "auc": roc_auc_score(y_test, y_prob_nn),
    },
}

import pandas as pd
metrics_df = pd.DataFrame(results).T
print(metrics_df)

plt.figure(figsize=(6,4))
plt.bar(metrics_df.index, metrics_df["acc"])
plt.ylabel("Accuracy")
plt.ylim(0, 1)
plt.title("Model Accuracy Comparison")
plt.show()

plt.figure(figsize=(6,4))
plt.plot(fpr_log, tpr_log, label=f"LogReg (AUC={roc_auc_score(y_test, y_prob_log):.2f})")
plt.plot(fpr_nn, tpr_nn, label=f"NeuralNet (AUC={roc_auc_score(y_test, y_prob_nn):.2f})")
plt.plot([0,1], [0,1], "k--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curves")
plt.legend()
plt.show()